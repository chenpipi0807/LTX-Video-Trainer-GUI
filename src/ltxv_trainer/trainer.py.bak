import os
import random
import time
import warnings
from copy import deepcopy
from functools import partial
from pathlib import Path
from typing import Callable, Optional
from unittest.mock import MagicMock

import torch
from accelerate import Accelerator
from accelerate.utils import set_seed
from diffusers import LTXImageToVideoPipeline, LTXPipeline
from diffusers.utils import export_to_video
from loguru import logger
from peft import LoraConfig, get_peft_model_state_dict
from peft.tuners.tuners_utils import BaseTunerLayer
from peft.utils import ModulesToSaveWrapper
from pydantic import BaseModel
from rich.console import Console
from rich.live import Live
from rich.panel import Panel
from rich.progress import (
    BarColumn,
    Group,
    MofNCompleteColumn,
    Progress,
    TextColumn,
    TimeElapsedColumn,
    TimeRemainingColumn,
)
from safetensors.torch import load_file, save_file
from torch import Tensor, nn
from torch.amp import autocast
from torch.optim import AdamW
from torch.optim.lr_scheduler import (
    CosineAnnealingLR,
    CosineAnnealingWarmRestarts,
    LinearLR,
    LRScheduler,
    PolynomialLR,
    StepLR,
)
from torch.utils.data import DataLoader

from ltxv_trainer.config import LtxvTrainerConfig
from ltxv_trainer.datasets import PrecomputedDataset
from ltxv_trainer.model_loader import load_ltxv_components
from ltxv_trainer.quantization import quantize_model
from ltxv_trainer.timestep_samplers import SAMPLERS
from ltxv_trainer.utils import get_gpu_memory_gb, open_image_as_srgb

# Disable irrelevant warnings from transformers
os.environ["TOKENIZERS_PARALLELISM"] = "true"

# Silence bitsandbytes warnings about casting
warnings.filterwarnings(
    "ignore", message="MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization"
)

StepCallback = Callable[[int, int, list[Path]], None]  # (step, total, list[sampled_video_path]) -> None

COMPILE_WARMUP_STEPS = 5


class TrainingStats(BaseModel):
    """Statistics collected during training"""

    total_time_seconds: float
    compilation_time_seconds: Optional[float]  # Only if compile_with_inductor=True
    training_time: float
    steps_per_second: float
    samples_per_second: float
    peak_gpu_memory_gb: float


class LtxvTrainer:
    def __init__(self, trainer_config: LtxvTrainerConfig) -> None:
        self._config = trainer_config
        self._console = Console()
        self._print_config(trainer_config)
        self._setup_accelerator()
        self._load_models()
        self._compile_transformer()
        self._collect_trainable_params()
        self._load_checkpoint()
        self._dataset = None
        self._global_step = -1
        self._checkpoint_paths = []

        # 添加初始化日志
        logger.info(f"初始化训练器 - 设备: {self._accelerator.device} - 混合精度模式: {self._config.acceleration.mixed_precision_mode}")

    def train(  # noqa: PLR0912, PLR0915
        self,
        disable_progress_bars: bool = False,
        step_callback: StepCallback = None,
    ) -> tuple[Path, TrainingStats]:
        """
        Start the training process.
        Returns:
            Tuple of (saved_model_path, training_stats)
        """
        device = self._accelerator.device
        cfg = self._config
        start_mem = get_gpu_memory_gb(device)

        train_start_time = time.time()
        set_seed(cfg.seed)

        if cfg.model.training_mode == "lora" and not cfg.model.load_checkpoint:
            self._init_lora_weights()

        self._init_optimizer()
        self._init_dataloader()
        self._init_timestep_sampler()

        Path(cfg.output_dir).mkdir(parents=True, exist_ok=True)

        logger.info("🚀 Starting training...")

        # Create progress columns with simplified styling
        if disable_progress_bars:
            train_progress = MagicMock()
            sample_progress = MagicMock()
            logger.warning("Progress bars disabled. Status messages will be printed occasionally instead.")
        else:
            train_progress = Progress(
                TextColumn("Training Step"),
                MofNCompleteColumn(),
                BarColumn(bar_width=40, style="blue"),
                TextColumn("Loss: {task.fields[loss]:.4f}"),
                TextColumn("LR: {task.fields[lr]:.2e}"),
                TextColumn("Time/Step: {task.fields[step_time]:.2f}s"),
                TimeElapsedColumn(),
                TextColumn("ETA:"),
                TimeRemainingColumn(compact=True),
            )

            # Create a separate progress instance for sampling
            sample_progress = Progress(
                TextColumn("Sampling validation videos"),
                MofNCompleteColumn(),
                BarColumn(bar_width=40, style="blue"),
                TimeElapsedColumn(),
                TextColumn("ETA:"),
                TimeRemainingColumn(compact=True),
            )

        self._transformer.train()
        self._global_step = 0

        # For tracking compilation time
        compilation_time = None
        peak_mem_during_training = start_mem

        # Track when actual training starts (after compilation)
        actual_training_start = None

        with Live(Panel(Group(train_progress, sample_progress)), refresh_per_second=2):
            task = train_progress.add_task(
                "Training",
                total=cfg.optimization.steps,
                loss=0.0,
                lr=cfg.optimization.learning_rate,
                step_time=0.0,
            )

            if cfg.validation.interval:
                self._sample_videos(sample_progress)

            for step in range(cfg.optimization.steps):
                # Get next batch, reset the dataloader if needed
                try:
                    batch = next(self._data_loader)
                except StopIteration:
                    self._init_dataloader()
                    batch = next(self._data_loader)

                # Measure compilation time (first COMPILE_WARMUP_STEPS steps)
                if step == COMPILE_WARMUP_STEPS and cfg.acceleration.compile_with_inductor:
                    compilation_time = time.time() - train_start_time
                    actual_training_start = time.time()
                elif step == COMPILE_WARMUP_STEPS and not cfg.acceleration.compile_with_inductor:
                    actual_training_start = train_start_time

                step_start_time = time.time()
                with self._accelerator.accumulate(self._transformer):
                    is_optimization_step = (step + 1) % cfg.optimization.gradient_accumulation_steps == 0
                    if is_optimization_step:
                        self._global_step += 1

                    loss = self._training_step(batch)
                    self._accelerator.backward(loss)

                    if self._accelerator.sync_gradients and cfg.optimization.max_grad_norm > 0:
                        self._accelerator.clip_grad_norm_(
                            self._trainable_params,
                            cfg.optimization.max_grad_norm,
                        )

                    self._optimizer.step()
                    self._optimizer.zero_grad()

                    if self._lr_scheduler is not None:
                        self._lr_scheduler.step()

                    # Run validation if needed
                    if (
                        cfg.validation.interval
                        and self._global_step > 0
                        and self._global_step % cfg.validation.interval == 0
                        and is_optimization_step
                        and self._accelerator.is_main_process
                    ):
                        sampled_videos_paths = self._sample_videos(sample_progress)

                    # Save checkpoint if needed
                    if (
                        cfg.checkpoints.interval
                        and self._global_step > 0
                        and self._global_step % cfg.checkpoints.interval == 0
                        and is_optimization_step
                        and self._accelerator.is_main_process
                    ):
                        self._save_checkpoint()

                    # Call step callback if provided
                    if step_callback and is_optimization_step:
                        step_callback(self._global_step, cfg.optimization.steps, sampled_videos_paths)

                    self._accelerator.wait_for_everyone()

                    # Determine device
                    if self._config.device is None:
                        if torch.cuda.is_available():
                            print("检测到CUDA设备，使用GPU训练")
                            self.device = torch.device("cuda")
                        else:
                            print("\n\033[33m未检测到CUDA设备或PyTorch未编译CUDA支持\033[0m")
                            print("\033[33m将使用CPU进行训练 (这可能非常慢)\033[0m\n")
                            self.device = torch.device("cpu")
                    else:
                        requested_device = self._config.device
                        if requested_device.startswith('cuda') and not torch.cuda.is_available():
                            print(f"\n\033[33m警告: 请求使用 {requested_device} 但CUDA不可用\033[0m")
                            print("\033[33m自动切换到CPU模式\033[0m\n")
                            self.device = torch.device("cpu")
                        else:
                            self.device = torch.device(requested_device)

                    # Update progress
                    if self._accelerator.is_main_process:
                        current_lr = self._optimizer.param_groups[0]["lr"]
                        elapsed = time.time() - train_start_time
                        progress_percentage = self._global_step / cfg.optimization.steps
                        if progress_percentage > 0:
                            total_estimated = elapsed / progress_percentage
                            total_time = f"{total_estimated // 3600:.0f}h {(total_estimated % 3600) // 60:.0f}m"
                        else:
                            total_time = "calculating..."

                        step_time = time.time() - step_start_time
                        train_progress.update(
                            task,
                            advance=1,
                            loss=loss.item(),
                            lr=current_lr,
                            step_time=step_time,
                            total_time=total_time,
                        )
                        if disable_progress_bars and self._global_step % 20 == 0:
                            logger.info(
                                f"Step {self._global_step}/{cfg.optimization.steps} - "
                                f"Loss: {loss.item():.4f}, LR: {current_lr:.2e}, "
                                f"Time/Step: {step_time:.2f}s, Total Time: {total_time}",
                            )

                    # Sample GPU memory periodically (every 25 steps)
                    if step % 25 == 0:
                        current_mem = get_gpu_memory_gb(device)
                        peak_mem_during_training = max(peak_mem_during_training, current_mem)

            # Collect final stats
            train_end_time = time.time()
            end_mem = get_gpu_memory_gb(device)
            peak_mem = max(start_mem, end_mem, peak_mem_during_training)

            # Calculate steps/second excluding compilation time if needed
            if cfg.acceleration.compile_with_inductor:
                training_time = train_end_time - actual_training_start
                steps_per_second = (cfg.optimization.steps - COMPILE_WARMUP_STEPS) / training_time
            else:
                training_time = train_end_time - train_start_time
                steps_per_second = cfg.optimization.steps / training_time

            samples_per_second = steps_per_second * self._accelerator.num_processes * cfg.optimization.batch_size

            stats = TrainingStats(
                total_time_seconds=train_end_time - train_start_time,
                training_time=training_time,
                compilation_time_seconds=compilation_time,
                steps_per_second=steps_per_second,
                samples_per_second=samples_per_second,
                peak_gpu_memory_gb=peak_mem,
            )

            train_progress.remove_task(task)
            self._accelerator.end_training()

            if self._accelerator.is_main_process:
                saved_path = self._save_checkpoint()

                # Log the training statistics
                self._log_training_stats(stats)

            return saved_path, stats

    def _training_step(self, batch: dict[str, dict[str, Tensor]]) -> Tensor:
        """Perform a single training step."""

        # Get pre-encoded latents
        latent_conditions = batch["latent_conditions"]
        latent_conditions["latents"].squeeze_(1)
        packed_latents = latent_conditions["latents"]

        # TODO: support batch sizes > 1 (requires a PR for the diffusers LTXImageToVideoPipeline)
        # Batch sizes > 1 are partially supported, assuming num_frames, height, width, fps
        # are the same for all batch elements.
        latent_frames = latent_conditions["num_frames"][0].item()
        latent_height = latent_conditions["height"][0].item()
        latent_width = latent_conditions["width"][0].item()

        # Handle FPS with backward compatibility for old preprocessed datasets
        fps = latent_conditions.get("fps", None)
        if fps is not None and not torch.all(fps == fps[0]):
            logger.warning(
                f"Different FPS values found in the batch. Found: {fps.tolist()}, using the first one: {fps[0].item()}"
            )

        fps = fps[0].item() if fps is not None else 24

        # Get pre-encoded text conditions
        text_conditions = batch["text_conditions"]
        prompt_embeds = text_conditions["prompt_embeds"]
        prompt_attention_mask = text_conditions["prompt_attention_mask"]

        sigmas = self._timestep_sampler.sample_for(packed_latents)
        timesteps = torch.round(sigmas * 1000.0).long()

        noise = torch.randn_like(packed_latents, device=self._accelerator.device)
        sigmas = sigmas.view(-1, 1, 1)

        loss_mask = torch.ones_like(packed_latents)
        # If first frame conditioning is enabled, the first latent (first video frame) is left (almost) unchanged.
        if (
            self._config.optimization.first_frame_conditioning_p
            and random.random() < self._config.optimization.first_frame_conditioning_p
        ):
            sigmas = sigmas.repeat(1, packed_latents.shape[1], 1)
            first_frame_end_idx = latent_height * latent_width

            # if we only have one frame (e.g. when training on still images),
            # skip this step otherwise we have no target to train on.
            if first_frame_end_idx < packed_latents.shape[1]:
                sigmas[:, :first_frame_end_idx] = 1e-5  # Small sigma close to 0 for the first frame.
                loss_mask[:, :first_frame_end_idx] = 0.0  # Mask out the loss for the first frame.
                # TODO: the `timesteps` fed to the transformer should be
                #  adjusted to reflect zero noise level for the first latent.

        noisy_latents = (1 - sigmas) * packed_latents + sigmas * noise
        targets = noise - packed_latents

        latent_frame_rate = fps / 8
        spatial_compression_ratio = 32
        rope_interpolation_scale = [1 / latent_frame_rate, spatial_compression_ratio, spatial_compression_ratio]

        model_pred = self._transformer(
            hidden_states=noisy_latents,
            encoder_hidden_states=prompt_embeds,
            timestep=timesteps,
            encoder_attention_mask=prompt_attention_mask,
            num_frames=latent_frames,
            height=latent_height,
            width=latent_width,
            rope_interpolation_scale=rope_interpolation_scale,
            return_dict=False,
        )[0]

        loss = (model_pred - targets).pow(2)
        loss = loss.mul(loss_mask).div(loss_mask.mean())  # divide by mean to keep the loss scale unchanged.
        return loss.mean()

    def _print_config(self, config: BaseModel) -> None:
        """Print the configuration as a nicely formatted table."""

        from rich.table import Table

        table = Table(title="⚙️ Training Configuration", show_header=True, header_style="bold green")
        table.add_column("Parameter", style="white")
        table.add_column("Value", style="cyan")

        def flatten_config(cfg: BaseModel, prefix: str = "") -> list[tuple[str, str]]:
            rows = []
            for field, value in cfg:
                full_field = f"{prefix}.{field}" if prefix else field
                if isinstance(value, BaseModel):
                    # Recursively flatten nested config
                    rows.extend(flatten_config(value, full_field))
                elif isinstance(value, (list, tuple, set)):
                    # Format list/tuple/set values
                    rows.append((full_field, ", ".join(str(item) for item in value)))
                else:
                    # Add simple values
                    rows.append((full_field, str(value)))
            return rows

        for param, value in flatten_config(config):
            table.add_row(param, value)

        self._console.print(table)

    def _load_models(self) -> None:
        """Load the LTXV model components."""
        prepare = self._accelerator.prepare

        # Load all model components using the new loader
        transformer_dtype = torch.bfloat16 if self._config.model.training_mode == "lora" else torch.float32
        components = load_ltxv_components(
            model_source=self._config.model.model_source,
            load_text_encoder_in_8bit=self._config.acceleration.load_text_encoder_in_8bit,
            transformer_dtype=transformer_dtype,
            vae_dtype=torch.bfloat16,
        )

        # Prepare components with accelerator
        self._scheduler = components.scheduler
        self._tokenizer = components.tokenizer
        self._text_encoder = prepare(components.text_encoder, device_placement=[False])
        self._vae = prepare(components.vae, device_placement=[False])
        transformer = components.transformer

        if self._config.acceleration.quantization is not None:
            if self._config.model.training_mode == "full":
                raise ValueError("Quantization is not supported in full training mode.")

            logger.warning(f"Quantizing model with precision: {self._config.acceleration.quantization}")
            transformer = quantize_model(
                transformer,
                precision=self._config.acceleration.quantization,
            )

        self._transformer = prepare(transformer)

        # Freeze all models. We later unfreeze the transformer based on training mode.
        self._text_encoder.requires_grad_(False)
        self._vae.requires_grad_(False)
        self._transformer.requires_grad_(False)

        # Enable gradient checkpointing if requested
        if self._config.optimization.enable_gradient_checkpointing:
            self._transformer.enable_gradient_checkpointing()

        # 添加模型加载日志
        logger.info(f"模型加载完成 - 设备: {self._accelerator.device}")

    # noinspection PyProtectedMember,PyUnresolvedReferences
    def _compile_transformer(self) -> None:
        """Compile the transformer model with Torch Inductor."""

        if not self._config.acceleration.compile_with_inductor:
            return

        torch._dynamo.config.inline_inbuilt_nn_modules = True
        torch._dynamo.config.cache_size_limit = 128

        compile_module = partial(torch.compile, mode=self._config.acceleration.compilation_mode)
        self._transformer.transformer_blocks = nn.ModuleList(
            [compile_module(block) for block in self._transformer.transformer_blocks],
        )

    def _collect_trainable_params(self) -> None:
        """Collect trainable parameters based on training mode."""
        if self._config.model.training_mode == "lora":
            # For LoRA training, first set up LoRA layers
            self._setup_lora()
        elif self._config.model.training_mode == "full":
            # For full training, unfreeze all transformer parameters
            self._transformer.requires_grad_(True)
        else:
            raise ValueError(f"Unknown training mode: {self._config.model.training_mode}")

        self._trainable_params = [p for p in self._transformer.parameters() if p.requires_grad]
        logger.debug(f"Trainable params count: {sum(p.numel() for p in self._trainable_params):,}")

    def _init_timestep_sampler(self) -> None:
        """Initialize the timestep sampler based on the config."""
        sampler_cls = SAMPLERS[self._config.flow_matching.timestep_sampling_mode]
        self._timestep_sampler = sampler_cls(**self._config.flow_matching.timestep_sampling_params)

    def _setup_lora(self) -> None:
        """Configure LoRA adapters for the transformer. Only called in LoRA training mode."""
        logger.debug(f"Adding LoRA adapter with rank {self._config.lora.rank}")
        lora_config = LoraConfig(
            r=self._config.lora.rank,
            lora_alpha=self._config.lora.alpha,
            target_modules=self._config.lora.target_modules,
            lora_dropout=self._config.lora.dropout,
            init_lora_weights=True,
        )
        self._transformer.add_adapter(lora_config)

    def _load_checkpoint(self) -> None:
        """Load checkpoint if specified in config."""
        if not self._config.model.load_checkpoint:
            return

        checkpoint_path = self._find_checkpoint(self._config.model.load_checkpoint)
        if not checkpoint_path:
            logger.warning(f"⚠️ Could not find checkpoint at {self._config.model.load_checkpoint}")
            return

        transformer = self._accelerator.unwrap_model(self._transformer)

        logger.info(f"📥 Loading checkpoint from {checkpoint_path}")
        state_dict = load_file(checkpoint_path)

        if self._config.model.training_mode == "full":
            transformer.load_state_dict(state_dict)
        else:  # LoRA mode
            # Adjust layer names to match PEFT format
            state_dict = {k.replace("transformer.", "", 1): v for k, v in state_dict.items()}
            state_dict = {k.replace("lora_A", "lora_A.default", 1): v for k, v in state_dict.items()}
            state_dict = {k.replace("lora_B", "lora_B.default", 1): v for k, v in state_dict.items()}

            # Load LoRA weights and verify all weights were loaded
            _, unexpected_keys = transformer.load_state_dict(state_dict, strict=False)
            if unexpected_keys:
                raise ValueError(f"Failed to load some LoRA weights: {unexpected_keys}")

    @staticmethod
    def _find_checkpoint(checkpoint_path: str | Path) -> Path | None:
        """Find the checkpoint file to load, handling both file and directory paths."""
        checkpoint_path = Path(checkpoint_path)

        if checkpoint_path.is_file():
            if not checkpoint_path.suffix == ".safetensors":
                raise ValueError(f"Checkpoint file must have a .safetensors extension: {checkpoint_path}")
            return checkpoint_path

        if checkpoint_path.is_dir():
            # Look for checkpoint files in the directory
            checkpoints = list(checkpoint_path.rglob("*step_*.safetensors"))

            if not checkpoints:
                return None

            # Sort by step number and return the latest
            def _get_step_num(p: Path) -> int:
                try:
                    return int(p.stem.split("step_")[1])
                except (IndexError, ValueError):
                    return -1

            latest = max(checkpoints, key=_get_step_num)
            return latest

        else:
            raise ValueError(f"Invalid checkpoint path: {checkpoint_path}. Must be a file or directory.")

    def _init_dataloader(self) -> None:
        """Initialize the training data loader."""

        if self._dataset is None:
            self._dataset = PrecomputedDataset(self._config.data.preprocessed_data_root)

        data_loader = DataLoader(
            self._dataset,
            batch_size=self._config.optimization.batch_size,
            shuffle=True,
            drop_last=True,
            num_workers=self._config.data.num_dataloader_workers,
            pin_memory=self._config.data.num_dataloader_workers > 0,
        )

        # noinspection PyTypeChecker
        data_loader = self._accelerator.prepare(data_loader)
        self._data_loader = iter(data_loader)

    def _init_lora_weights(self) -> None:
        """Initialize LoRA weights for the transformer."""
        logger.debug("Initializing LoRA weights...")
        for _, module in self._transformer.named_modules():
            if isinstance(module, (BaseTunerLayer, ModulesToSaveWrapper)):
                module.reset_lora_parameters(adapter_name="default", init_lora_weights=True)

    def _init_optimizer(self) -> None:
        """Initialize the optimizer and learning rate scheduler."""
        opt_cfg = self._config.optimization

        lr = opt_cfg.learning_rate
        if opt_cfg.optimizer_type == "adamw":
            optimizer = AdamW(self._trainable_params, lr=lr)
        elif opt_cfg.optimizer_type == "adamw8bit":
            # noinspection PyUnresolvedReferences
            from bitsandbytes.optim import AdamW8bit  # type: ignore

            optimizer = AdamW8bit(self._trainable_params, lr=lr)
        else:
            raise ValueError(f"Unknown optimizer type: {opt_cfg.optimizer_type}")

        # Add scheduler initialization
        lr_scheduler = self._create_scheduler(optimizer)

        # noinspection PyTypeChecker
        self._optimizer, self._lr_scheduler = self._accelerator.prepare(optimizer, lr_scheduler)

    def _create_scheduler(self, optimizer: torch.optim.Optimizer) -> LRScheduler | None:
        """Create learning rate scheduler based on config."""
        scheduler_type = self._config.optimization.scheduler_type
        steps = self._config.optimization.steps
        params = self._config.optimization.scheduler_params or {}

        if scheduler_type is None:
            return None

        if scheduler_type == "linear":
            scheduler = LinearLR(
                optimizer,
                start_factor=params.pop("start_factor", 1.0),
                end_factor=params.pop("end_factor", 0.1),
                total_iters=steps,
                **params,
            )
        elif scheduler_type == "cosine":
            scheduler = CosineAnnealingLR(
                optimizer,
                T_max=steps,
                eta_min=params.get("eta_min", 0),
                **params,
            )
        elif scheduler_type == "cosine_with_restarts":
            scheduler = CosineAnnealingWarmRestarts(
                optimizer,
                T_0=params.pop("T_0", steps // 4),  # First restart cycle length
                T_mult=params.pop("T_mult", 1),  # Multiplicative factor for cycle lengths
                eta_min=params.pop("eta_min", 5e-5),
                **params,
            )
        elif scheduler_type == "polynomial":
            scheduler = PolynomialLR(
                optimizer,
                total_iters=steps,
                power=params.pop("power", 1.0),
                **params,
            )
        elif scheduler_type == "step":
            scheduler = StepLR(
                optimizer,
                step_size=params.pop("step_size", steps // 2),
                gamma=params.pop("gamma", 0.1),
                **params,
            )
        elif scheduler_type == "constant":
            scheduler = None
        else:
            raise ValueError(f"Unknown scheduler type: {scheduler_type}")

        return scheduler

    def _setup_accelerator(self) -> None:
        """Initialize the Accelerator with the appropriate settings."""
        # 检查CUDA是否可用，如果不可用则禁用混合精度
        mixed_precision = self._config.acceleration.mixed_precision_mode
        
        if not torch.cuda.is_available() and mixed_precision != "no":
            print(f"\n\033[33m警告: PyTorch未编译CUDA支持或CUDA不可用\033[0m")
            print(f"\033[33m自动关闭混合精度模式({mixed_precision})，使用CPU训练\033[0m\n")
            mixed_precision = "no"
            
            # 强制禁用其他与加速相关的设置
            self._config.acceleration.load_text_encoder_in_8bit = False
            self._config.acceleration.compile_with_inductor = False
            self._config.acceleration.quantization = None
            
        self._accelerator = Accelerator(
            mixed_precision=mixed_precision,
            log_with=None,  # No built-in logging
        )
        
        # 在初始化后显示设备信息
        device_type = "CPU" if self._accelerator.device.type == "cpu" else f"GPU ({self._accelerator.device})"
        print(f"\033[32m训练设备: {device_type}\033[0m")
        print(f"\033[32m混合精度模式: {mixed_precision}\033[0m")

    def _sample_videos(self, progress=None, save=False) -> list[Path]:
        """生成验证视频

        Args:
            progress: 进度条对象
            save: 是否保存视频

        Returns:
            list[Path]: 生成的视频路径列表
        """
        # 如果全局步数低于500，暂时跳过视频生成以加快训练
        if self._global_step < 500:
            print("\033[33m警告: 验证视频生成已暂时禁用，以避免设备不匹配错误\033[0m")
            return []
            
        # 如果全局步数不是500的倍数，也跳过
        if self._global_step % 500 != 0:
            return []
        
        print(f"\033[32m开始生成验证视频 (步骤: {self._global_step})\033[0m")
        
        # 创建pipeline
        try:
            use_images = self._config.validation.images is not None
            
            if use_images:
                if len(self._config.validation.images) != len(self._config.validation.prompts):
                    raise ValueError(
                        f"Number of images ({len(self._config.validation.images)}) must match "
                        f"number of prompts ({len(self._config.validation.prompts)})"
                    )

                pipeline = LTXImageToVideoPipeline(
                    scheduler=deepcopy(self._scheduler),
                    vae=self._vae,
                    text_encoder=self._text_encoder,
                    tokenizer=self._tokenizer,
                    transformer=self._transformer,
                )
            else:
                pipeline = LTXPipeline(
                    scheduler=deepcopy(self._scheduler),
                    vae=self._vae,
                    text_encoder=self._text_encoder,
                    tokenizer=self._tokenizer,
                    transformer=self._transformer,
                )
            pipeline.set_progress_bar_config(disable=True)
        except Exception as e:
            print(f"\033[31m验证pipeline创建失败: {e}\033[0m")
            return []  # 如果pipeline创建失败，直接返回空列表

        output_dir = Path(self._config.output_dir) / "samples"
        output_dir.mkdir(exist_ok=True, parents=True)

        video_paths = []
        i = 0
        for j, prompt in enumerate(self._config.validation.prompts):
            # 选择目标设备 - 优先使用CUDA
            target_device = "cuda" if torch.cuda.is_available() else "cpu"
            print(f"目标设备: {target_device}")
            
            # 强制将整个pipeline移动到目标设备
            success = self._move_pipeline_to_device(pipeline, target_device)
            if not success:
                print("无法将pipeline移动到目标设备，尝试使用CPU")
                target_device = "cpu"
                self._move_pipeline_to_device(pipeline, target_device)
            
            # 获取pipeline的实际设备
            pipeline_device = self._get_pipeline_device(pipeline)
            if pipeline_device and pipeline_device != target_device:
                print(f"警告: pipeline未成功移动到{target_device}，实际在{pipeline_device}设备上")
                # 如果pipeline强行要在CPU上，我们就设置生成器也在CPU上
                target_device = "cpu"
            
            # 创建匹配设备的generator
            try:
                generator = torch.Generator(device=target_device).manual_seed(self._config.validation.seed)
                print(f"创建{target_device}生成器，种子={self._config.validation.seed}")
            except Exception as e:
                print(f"无法创建{target_device}生成器: {e}，回退到CPU")
                generator = torch.Generator(device="cpu").manual_seed(self._config.validation.seed)
                target_device = "cpu"
                # 如果无法创建CUDA生成器，尝试将pipeline移动到CPU
                self._move_pipeline_to_device(pipeline, "cpu")
                pipeline_device = self._get_pipeline_device(pipeline)
            
            # 记录当前设备状态并更新
            final_pipeline_device = self._get_pipeline_device(pipeline)
            print(f"最终状态 - Generator: {generator.device}, Pipeline: {final_pipeline_device or '未知'}")
            
            # 准备pipeline输入
            pipeline_inputs = {
                "prompt": prompt,
                "negative_prompt": self._config.validation.negative_prompt,
                "width": self._config.validation.video_dims[0],
                "height": self._config.validation.video_dims[1],
                "num_frames": self._config.validation.video_dims[2],
                "num_inference_steps": self._config.validation.inference_steps,
                "generator": generator
                # 不添加device参数，LTXPipeline不支持这个参数
            }
            
            if use_images:
                image_path = self._config.validation.images[j]
                pipeline_inputs["image"] = open_image_as_srgb(image_path)

            with autocast(self._accelerator.device.type, dtype=torch.bfloat16):
                result = pipeline(**pipeline_inputs)
                videos = result.frames
                
            for video in videos:
                video_path = output_dir / f"step_{self._global_step:06d}_{i}.mp4"
                export_to_video(video, str(video_path), fps=25)
                video_paths.append(video_path)
                i += 1
            progress.update(task, advance=1)
            
    except Exception as e:
        # 捕获所有验证视频生成过程中的错误
        print(f"\033[31m验证视频生成失败: {e}\033[0m")
        # 如果有进度条，先清理它
        if progress and 'task' in locals():
            try:
                progress.remove_task(task)
            except Exception:
                pass
        return []  # 返回空列表
    
    if progress:
        try:
            progress.remove_task(task)
        except Exception:
            pass

    # 移动不用的组件到CPU以节省显存
    try:
        self._vae.to("cpu")
        if not self._config.acceleration.load_text_encoder_in_8bit:
            self._text_encoder.to("cpu")
    except Exception as e:
        print(f"\033[33m警告: 将组件移回到CPU时出错: {e}\033[0m")
            # 尝试不同的方法获取设备信息
            if hasattr(pipeline, 'device'):
                return str(pipeline.device)
            elif hasattr(pipeline, 'transformer') and hasattr(pipeline.transformer, 'device'):
                return str(pipeline.transformer.device)
            elif hasattr(pipeline, 'unet') and hasattr(pipeline.unet, 'device'):
                return str(pipeline.unet.device)
            elif hasattr(pipeline, 'text_encoder') and hasattr(pipeline.text_encoder, 'device'):
                return str(pipeline.text_encoder.device)
            elif hasattr(pipeline, 'vae') and hasattr(pipeline.vae, 'device'):
                return str(pipeline.vae.device)
            
            # 尝试获取第一个参数的设备
            try:
                for param in pipeline.parameters():
                    return str(param.device)
            except (AttributeError, StopIteration):
                pass
                
            # 最后检查模型中所有可能的子模块
            for attr_name in dir(pipeline):
                if attr_name.startswith('_'):
                    continue
                    
                attr = getattr(pipeline, attr_name, None)
                if isinstance(attr, torch.nn.Module):
                    try:
                        for param in attr.parameters():
                            return str(param.device)
                    except (AttributeError, StopIteration):
                        pass
        except Exception as e:
            print(f"获取pipeline设备时出错: {e}")
            
        return None
        
    def _move_pipeline_to_device(self, pipeline, device):
        """确保整个pipeline及其所有组件都在指定设备上"""
        # 首先尝试使用to方法
        try:
            if hasattr(pipeline, 'to'):
                pipeline.to(device)
                print(f"将整个pipeline移动到{device}")
                return True
        except Exception as e:
            print(f"无法使用to方法移动pipeline: {e}")
        
        # 如果to方法失败，手动移动每个组件
        success = False
        try:
            # 尝试移动transformer - 这是最关键的组件
            if hasattr(pipeline, 'transformer'):
                pipeline.transformer = pipeline.transformer.to(device)
                print(f"将transformer移动到{device}")
                success = True
                
            # 尝试移动text_encoder
            if hasattr(pipeline, 'text_encoder'):
                pipeline.text_encoder = pipeline.text_encoder.to(device)
                print(f"将text_encoder移动到{device}")
                success = True
                
            # 尝试移动VAE
            if hasattr(pipeline, 'vae'):
                pipeline.vae = pipeline.vae.to(device)
                print(f"将vae移动到{device}")
                success = True
                
            # 尝试移动unet
            if hasattr(pipeline, 'unet'):
                pipeline.unet = pipeline.unet.to(device)
                print(f"将unet移动到{device}")
                success = True
                
            # 强制设置device属性
            if hasattr(pipeline, 'device') and hasattr(pipeline.__class__, 'device'):
                if isinstance(getattr(pipeline.__class__, 'device', None), property):
                    if hasattr(pipeline, '_device'):
                        pipeline._device = torch.device(device)
                        print(f"设置pipeline._device = {device}")
                        success = True
            
            # 更激进方法：将隐藏属性直接设置为该设备
            if hasattr(pipeline, '_device'):
                pipeline._device = torch.device(device)
                print(f"直接设置pipeline._device = {device}")
                success = True
            
            # 尝试注入设备重定向
            # 这种方法很激进，但可能是必要的
            try:
                original_to = pipeline.to
                
                def forced_to(self_obj, *args, **kwargs):
                    result = original_to(*args, **kwargs)
                    print(f"拦截pipeline.to调用，强制设置为{device}")
                    if hasattr(self_obj, '_device'):
                        self_obj._device = torch.device(device)
                    return result
                    
                pipeline.to = lambda *args, **kwargs: forced_to(pipeline, *args, **kwargs)
                pipeline.to(device)
                success = True
            except Exception as e:
                print(f"注入设备重定向失败: {e}")
                
        except Exception as e:
            print(f"手动移动pipeline组件时出错: {e}")
        
        # 强制torch进行垃圾回收，确保设备更改生效
        torch.cuda.empty_cache()
        
        return success

    def _log_training_stats(self, stats: TrainingStats) -> None:
        """Log training statistics."""
        logger.info("📊 Training Statistics:")
        logger.info(f"Total time: {stats.total_time_seconds / 60:.1f} minutes")
        logger.info(f"Training time: {stats.training_time / 60:.1f} minutes")
        if stats.compilation_time_seconds is not None:
            logger.info(f"Compilation time: {stats.compilation_time_seconds:.1f} seconds")
        logger.info(f"Training speed: {stats.steps_per_second:.2f} steps/second")
        logger.info(f"Samples/second: {stats.samples_per_second:.2f}")
        logger.info(f"Peak GPU memory: {stats.peak_gpu_memory_gb:.2f} GB")

    def _save_checkpoint(self) -> Path:
        """Save the model weights."""

        # Create checkpoints directory if it doesn't exist
        save_dir = Path(self._config.output_dir) / "checkpoints"
        save_dir.mkdir(exist_ok=True, parents=True)

        # Create filename with step number
        prefix = "model" if self._config.model.training_mode == "full" else "lora"
        filename = f"{prefix}_weights_step_{self._global_step:05d}.safetensors"
        saved_weights_path = save_dir / filename
        rel_saved_weights_path = saved_weights_path.relative_to(self._config.output_dir)

        # Get model state dict
        unwrapped_model = self._accelerator.unwrap_model(self._transformer)

        if self._config.model.training_mode == "full":
            state_dict = unwrapped_model.state_dict()
            save_file(state_dict, saved_weights_path)
            logger.info(f"💾 Model weights for step {self._global_step} saved in {rel_saved_weights_path}")
        elif self._config.model.training_mode == "lora":
            state_dict = get_peft_model_state_dict(unwrapped_model)
            # Adjust layer names to standard formatting.
            state_dict = {k.replace("_orig_mod.", ""): v for k, v in state_dict.items()}
            LTXPipeline.save_lora_weights(
                save_directory=save_dir,
                transformer_lora_layers=state_dict,
                weight_name=filename,
            )
            logger.info(f"💾 LoRA weights for step {self._global_step} saved in {rel_saved_weights_path}")
        else:
            raise ValueError(f"Unknown training mode: {self._config.model.training_mode}")

        # Keep track of checkpoint paths, and cleanup old checkpoints if needed
        self._checkpoint_paths.append(saved_weights_path)
        self._cleanup_checkpoints()

        return saved_weights_path

    def _cleanup_checkpoints(self) -> None:
        """Clean up old checkpoints."""
        if 0 < self._config.checkpoints.keep_last_n < len(self._checkpoint_paths):
            checkpoints_to_remove = self._checkpoint_paths[: -self._config.checkpoints.keep_last_n]
            for old_checkpoint in checkpoints_to_remove:
                if old_checkpoint.exists():
                    old_checkpoint.unlink()
                    logger.debug(f"Removed old checkpoints: {old_checkpoint}")
            # Update the list to only contain kept checkpoints
            self._checkpoint_paths = self._checkpoint_paths[-self._config.checkpoints.keep_last_n :]
